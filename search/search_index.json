{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"What is DIGEN? Diverse and Generative ML benchmark (DIGEN) is a modern machine learning benchmark, which includes 40 datasets specially designed to differentiate the performance of some of the leading Machine Learning (ML) methods. DIGEN provides comprehensive information on the datasets, as well as on the performance of the leading ML methods. For each of the datasets, we provide a mathematical formula for the endpoint (ground truth) as well as the results of exploratory analysis, which includes feature correlation and histogram showing how binary endpoint was calculated. Each dataset comes also with Reveiver-Operating Characteristics (ROC), Precision-Recall (PRC) charts for tuned ML methods with 100 guided optimizations, as well as boxplot showing the expected performance of the methods tested on 100 different starting random seeds and with 100 optimizations for each random seed. Installation Installation of the following packages is required in order to use DIGEN: pandas>=1.05 numpy>=1.19.5 optuna>=2.4.0 scikit-learn>=0.22.2 importlib_resources In order to reproduce the computations, the following packages are also suggested: deap>=1.3 digen seaborn Matplotlib lightgbm>=3.1.1 xgboost>=1.3.2 requests argparse inspect How to install DIGEN? Download the latest release of DIGEN from PyPI: pip install digen Citing DIGEN DIGEN was developed by Patryk Orzechowski and Jason H. Moore at the University of Pennsylvania.","title":"Home"},{"location":"index.html#what-is-digen","text":"Diverse and Generative ML benchmark (DIGEN) is a modern machine learning benchmark, which includes 40 datasets specially designed to differentiate the performance of some of the leading Machine Learning (ML) methods. DIGEN provides comprehensive information on the datasets, as well as on the performance of the leading ML methods. For each of the datasets, we provide a mathematical formula for the endpoint (ground truth) as well as the results of exploratory analysis, which includes feature correlation and histogram showing how binary endpoint was calculated. Each dataset comes also with Reveiver-Operating Characteristics (ROC), Precision-Recall (PRC) charts for tuned ML methods with 100 guided optimizations, as well as boxplot showing the expected performance of the methods tested on 100 different starting random seeds and with 100 optimizations for each random seed.","title":"What is DIGEN?"},{"location":"index.html#installation","text":"Installation of the following packages is required in order to use DIGEN: pandas>=1.05 numpy>=1.19.5 optuna>=2.4.0 scikit-learn>=0.22.2 importlib_resources In order to reproduce the computations, the following packages are also suggested: deap>=1.3 digen seaborn Matplotlib lightgbm>=3.1.1 xgboost>=1.3.2 requests argparse inspect","title":"Installation"},{"location":"index.html#how-to-install-digen","text":"Download the latest release of DIGEN from PyPI: pip install digen","title":"How to install DIGEN?"},{"location":"index.html#citing-digen","text":"DIGEN was developed by Patryk Orzechowski and Jason H. Moore at the University of Pennsylvania.","title":"Citing DIGEN"},{"location":"benchmark.html","text":"How to use DIGEN? The most common usecase of using DIGEN is validating performance of a new method against the benchmark. In order to use DIGEN, we first import the package: from digen import Benchmark benchmark = Benchmark () The most common use case includes validation of a new method against DIGEN. First, we need to define our new classifier. For this example, we will use ExtraTreesClassifier from scikit-learn: from sklearn.ensemble import ExtraTreesClassifier est = ExtraTreesClassifier () In order to benchmark a method, we need to define its parameters and their values. Please set the expected range of hyper parameters for your method below. For details, please refer to Optuna def params_myParamScope(trial): params={ 'n_estimators' : trial.suggest_int('n_estimators',10,100), 'criterion' : trial.suggest_categorical(name='criterion',choices=['gini', 'entropy']), 'max_depth' : trial.suggest_int('max_depth', 1, 10), } return params After defining distributions and scope of each of the hyperparameters, all we need to do is to perform optimizations on each of the DIGEN datasets in order to fairly compare its performance against predefined methods: results=benchmark.optimize(est=est, parameter_scopes=params_myParamScope, storage='sqlite:///test.db', local_cache_dir='.') This command will initiate a benchmarking experiment with 200 optimizations per each of the datasets (it may take a while!). It is also possible to perform computations on a single dataset, e.g.: results=benchmark.optimize(est=est, dataset='digen8_4426', parameter_scopes=params_myParamScope, storage='sqlite:///test.db', local_cache_dir='.') DIGEN provides a comprehensive API with multiple visualizations, selections and full access and logging of the performed operations. For the reference, please check our Tutorial Advanced Tutorial . Naming convention Apart from numeric identifiers of the datasets, we are also using the following abbreviations for ML methods: X - XGBoost, L - LightGBM, G - Gradient Boosting, F - Random Forest, S - Support Vector Classifier, K - K-Nnearest Neighbors, R - Logistic Regression, D - Decision Tree. The abbreviations used in DIGEN reflect the performance of tuned ML methods on a dataset created with a specific seed. The name of each dataset starts with letters determining the order of the performances of the tuned ML methods included in the experiment. If two or more ML methods have the same score, a hyphen is used to separate them. Multiple datasets may have the same order of the methods. Storing ML experiments We have adapted the following strategy to ensure the unique identification of the experiments. Each mathematical formula was represented as a string extracted from DEAP and was hashed using an MD5 algorithm. The experiments are stored as studies in a SQLite dataset as Optuna storage (SQLite) in the following format digenNUM_SEED_method , where NUM stands for a dataset it, SEED - a random seed used as an initializer of the dataset and the methods, and method is a classifier that was used to analyze a dataset. In the supporting files within the package, some additional properties ma be found, such as methodUP and methodDOWN , which are two methods used to compete against each other, and hash that represents an MD5 shortcut of a mathematical formula. Hashing an equation doesn't guarantee a completely unique mathematical formula (e.g. swapping the operands in sum creates a different hash function, so as adding a meaningless addition of 0, or multiplication by 1). Nonetheless, it allows to identify redundancy and do not repeat steps that were already explored.","title":"Benchmark"},{"location":"benchmark.html#how-to-use-digen","text":"The most common usecase of using DIGEN is validating performance of a new method against the benchmark. In order to use DIGEN, we first import the package: from digen import Benchmark benchmark = Benchmark () The most common use case includes validation of a new method against DIGEN. First, we need to define our new classifier. For this example, we will use ExtraTreesClassifier from scikit-learn: from sklearn.ensemble import ExtraTreesClassifier est = ExtraTreesClassifier () In order to benchmark a method, we need to define its parameters and their values. Please set the expected range of hyper parameters for your method below. For details, please refer to Optuna def params_myParamScope(trial): params={ 'n_estimators' : trial.suggest_int('n_estimators',10,100), 'criterion' : trial.suggest_categorical(name='criterion',choices=['gini', 'entropy']), 'max_depth' : trial.suggest_int('max_depth', 1, 10), } return params After defining distributions and scope of each of the hyperparameters, all we need to do is to perform optimizations on each of the DIGEN datasets in order to fairly compare its performance against predefined methods: results=benchmark.optimize(est=est, parameter_scopes=params_myParamScope, storage='sqlite:///test.db', local_cache_dir='.') This command will initiate a benchmarking experiment with 200 optimizations per each of the datasets (it may take a while!). It is also possible to perform computations on a single dataset, e.g.: results=benchmark.optimize(est=est, dataset='digen8_4426', parameter_scopes=params_myParamScope, storage='sqlite:///test.db', local_cache_dir='.') DIGEN provides a comprehensive API with multiple visualizations, selections and full access and logging of the performed operations. For the reference, please check our Tutorial Advanced Tutorial .","title":"How to use DIGEN?"},{"location":"benchmark.html#naming-convention","text":"Apart from numeric identifiers of the datasets, we are also using the following abbreviations for ML methods: X - XGBoost, L - LightGBM, G - Gradient Boosting, F - Random Forest, S - Support Vector Classifier, K - K-Nnearest Neighbors, R - Logistic Regression, D - Decision Tree. The abbreviations used in DIGEN reflect the performance of tuned ML methods on a dataset created with a specific seed. The name of each dataset starts with letters determining the order of the performances of the tuned ML methods included in the experiment. If two or more ML methods have the same score, a hyphen is used to separate them. Multiple datasets may have the same order of the methods.","title":"Naming convention"},{"location":"benchmark.html#storing-ml-experiments","text":"We have adapted the following strategy to ensure the unique identification of the experiments. Each mathematical formula was represented as a string extracted from DEAP and was hashed using an MD5 algorithm. The experiments are stored as studies in a SQLite dataset as Optuna storage (SQLite) in the following format digenNUM_SEED_method , where NUM stands for a dataset it, SEED - a random seed used as an initializer of the dataset and the methods, and method is a classifier that was used to analyze a dataset. In the supporting files within the package, some additional properties ma be found, such as methodUP and methodDOWN , which are two methods used to compete against each other, and hash that represents an MD5 shortcut of a mathematical formula. Hashing an equation doesn't guarantee a completely unique mathematical formula (e.g. swapping the operands in sum creates a different hash function, so as adding a meaningless addition of 0, or multiplication by 1). Nonetheless, it allows to identify redundancy and do not repeat steps that were already explored.","title":"Storing ML experiments"},{"location":"faq.html","text":"Frequently asked questions (FAQ) I got a different AUROC value for one of the methods than the one in DIGEN. Is it a bug? No, it's not a bug. It is possible that the specific settings of ML method (even default ones) are better than the results obtained in hyper-parameters tuning process performed with Optuna. DIGEN does up to 200 hyper-parameter optimizations for each ML method. Although this is a decent number of trials, this does not guarantee that the results will be optimal, unless a Docker image is used. Are the results from DIGEN reproducible? Yes! In order to provide full reproducibility of the experiment across different platforms, we have included a Docker configuration files, allowing to build a container with specific libraries and configurations. Specifically, the following versions of the packages were used to assure reproducibility: numpy 1.19.5 sklearn 0.22.2.post1 xgboost 1.3.1 lightgbm 3.1.1 optuna 2.4.0 pandas==1.1.5 numpy==1.19.5 optuna==2.4.0 deap==1.3","title":"FAQ"},{"location":"faq.html#frequently-asked-questions-faq","text":"","title":"Frequently asked questions (FAQ)"},{"location":"faq.html#i-got-a-different-auroc-value-for-one-of-the-methods-than-the-one-in-digen-is-it-a-bug","text":"No, it's not a bug. It is possible that the specific settings of ML method (even default ones) are better than the results obtained in hyper-parameters tuning process performed with Optuna. DIGEN does up to 200 hyper-parameter optimizations for each ML method. Although this is a decent number of trials, this does not guarantee that the results will be optimal, unless a Docker image is used.","title":"I got a different AUROC value for one of the methods than the one in DIGEN. Is it a bug?"},{"location":"faq.html#are-the-results-from-digen-reproducible","text":"Yes! In order to provide full reproducibility of the experiment across different platforms, we have included a Docker configuration files, allowing to build a container with specific libraries and configurations. Specifically, the following versions of the packages were used to assure reproducibility: numpy 1.19.5 sklearn 0.22.2.post1 xgboost 1.3.1 lightgbm 3.1.1 optuna 2.4.0 pandas==1.1.5 numpy==1.19.5 optuna==2.4.0 deap==1.3","title":"Are the results from DIGEN reproducible?"},{"location":"methods.html","text":"Methods DIGEN benchmark emerged as the result of multilevel optimization starting from a duel between 2 ML methods with 15 different random seeds. As running ML method with default parameters doesn't necessarily reflect its actual performance, a complex strategy aimed at determining actual ML methods performance is performed. In the first stage, a two-factor optimization is used to find the optimal candidates that both maximize the difference between the 2 ML methods in terms of AUROC score as well as maximize the standard deviation of all the methods. The specific parameters of ML methods are chosen according to their distribution based on the recommendations from AutoSklearn 1 2 , Optuna 3 , Hyperopt 4 and expert knowledge. The best performing candidates from each of the duel are selected for thorough optimization of all the ML methods. In the second stage, all the selected candidates are optimized and benchmarked under the same conditions (200 optimizations). The third stage is simply filtering the results down to 40 datasets, which differentiate the performance of 8 ML the most. Algorithm Our goal in development of DIGEN was to provide a balanced datasets with N(0,1) distributed data and manipulate the endpoint based on the ML methods performance. The steps of the algorithm, which was used to determine the datesets are outlined below. HIBACHI Heuristic Identification of Biological Architectures for simulating Complex Hierarchical Interactions (HIBACHI) 5 is a method based on genetic programming (GP) which creates different mathematical formulas for the endpoint. The endpoints of each of the dataset is a mathematical formula, represented by a directed graph that contains variables (features of the dataset), terminals (values) as well as mathematical operators. Stage 1 - pairwise duels The pipeline starts with generating independent variables of the dataset using normal distribution N(0,1). Then, multiple candidates for dependent variable (endpoint) are generated using HIBACHI. After evaluating each of the instances, all the results are sorted and binarized, so that the half of the largest values is assigned '1' and half with lowest - '0'. After the endpoints are determined, the dataset is split into training and testing sets (80%/20%). Training part is further split into 10 folds. Optuna 3 is used to control the tuning of ML methods (see Algorithm 1). X_{train}, y_{train} \\gets next fold in 10-fold cross validation auc \\gets auc.append(est.predict(X_{test}, y_{test})) Both ML methods participating in the duel are optimized 10 times and their performance is averaged on 2 out of 10 folds. This follows the approach presented by pruned-cv 8 . If this difference is positive and significantly better than the previously found ones, all 8 ML methods are evaluated on full 10 folds to determine two optimization factors: difference in AUROC score between 2 competing methods as well as standard deviation between the scores of all 8 ML methods. Otherwise, the potential solution is pruned and invalidated. Having computed optimization factors for all candidates in a given iteration, Pareto-optimal candidates are selected using NSGA-III strategy 6 and corresponding GP-trees are mutated using genetic operators, such as mutations and crossovers. Algorithm stage 2 - thorough optimizations After completing all pairwise duels between ML methods using 15 different random seeds, an extended optimization using 200 combinations of hyper-parameters is performed in order to reduce bias from improperly settings the parameters. Finally, after selecting several hundreds of the most promising solutions, 100 different random seeds are tested, so that the results are independent on the choice of the random seed. The final step includes manual selection of the datasets, which in our opinion represent the most diverse results within the collection. Algorithm stage 3 - filtering the best datasets In the final steps, a replication of the results is considered. In this stage, for each of the formulas selected, 100 random seeds are used to create different datasets, which are later analyzed using the same model. Each of the datasets is used to run 100 optimizations of each of the 8 ML methods. Based on the results, the collection is narrowed down to 40 datasets. Operands In order to determine the dependent variable in each of the datasets in DIGEN40 we used Genetic Programming 7 . Using evolutionary algorithm, in each iteration multiple candidates are tested that feature the following operands: arithmetic: add , subtract , multiply , safe division (safeDiv) ; and relational: greater (or equal) than , lesser (or equal) than , equal , not equal , minimum and maximum . The DIGEN collection is diverse in terms of the choice of the operators included. Only two datasets use exactly the same set of operators (safe division and multiplication), but in different order (either first safeDiv than multiplication, or the opposite). References Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank Hutter. Efficient and robust automated machine learning. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2962\u20132970. Curran Associates, Inc., 2015. URL: http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf . \u21a9 Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. Auto-sklearn 2.0: the next generation. arXiv preprint arXiv:2007.04074 , 2020. \u21a9 Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: a next-generation hyperparameter optimization framework. In Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 2019. \u21a9 \u21a9 James Bergstra, Brent Komer, Chris Eliasmith, Dan Yamins, and David D Cox. Hyperopt: a python library for model selection and hyperparameter optimization. Computational Science & Discovery , 8 1 :014008, 2015. \u21a9 Jason H Moore, Randal S Olson, Peter Schmitt, Yong Chen, and Elisabetta Manduchi. How computational thought experiments can improve our understanding of the genetic architecture of common human diseases. In Artificial Life Conference Proceedings , 23\u201330. MIT Press, 2018. \u21a9 Kalyanmoy Deb and Himanshu Jain. An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: solving problems with box constraints. IEEE transactions on evolutionary computation , 18 4 :577\u2013601, 2013. \u21a9 John R Koza and John R Koza. Genetic programming: on the programming of computers by means of natural selection . Volume 1. MIT press, 1992. \u21a9 https://github.com/PiotrekGa/pruned-cv \u21a9","title":"Methods"},{"location":"methods.html#methods","text":"DIGEN benchmark emerged as the result of multilevel optimization starting from a duel between 2 ML methods with 15 different random seeds. As running ML method with default parameters doesn't necessarily reflect its actual performance, a complex strategy aimed at determining actual ML methods performance is performed. In the first stage, a two-factor optimization is used to find the optimal candidates that both maximize the difference between the 2 ML methods in terms of AUROC score as well as maximize the standard deviation of all the methods. The specific parameters of ML methods are chosen according to their distribution based on the recommendations from AutoSklearn 1 2 , Optuna 3 , Hyperopt 4 and expert knowledge. The best performing candidates from each of the duel are selected for thorough optimization of all the ML methods. In the second stage, all the selected candidates are optimized and benchmarked under the same conditions (200 optimizations). The third stage is simply filtering the results down to 40 datasets, which differentiate the performance of 8 ML the most.","title":"Methods"},{"location":"methods.html#algorithm","text":"Our goal in development of DIGEN was to provide a balanced datasets with N(0,1) distributed data and manipulate the endpoint based on the ML methods performance. The steps of the algorithm, which was used to determine the datesets are outlined below.","title":"Algorithm"},{"location":"methods.html#hibachi","text":"Heuristic Identification of Biological Architectures for simulating Complex Hierarchical Interactions (HIBACHI) 5 is a method based on genetic programming (GP) which creates different mathematical formulas for the endpoint. The endpoints of each of the dataset is a mathematical formula, represented by a directed graph that contains variables (features of the dataset), terminals (values) as well as mathematical operators.","title":"HIBACHI"},{"location":"methods.html#stage-1-pairwise-duels","text":"The pipeline starts with generating independent variables of the dataset using normal distribution N(0,1). Then, multiple candidates for dependent variable (endpoint) are generated using HIBACHI. After evaluating each of the instances, all the results are sorted and binarized, so that the half of the largest values is assigned '1' and half with lowest - '0'. After the endpoints are determined, the dataset is split into training and testing sets (80%/20%). Training part is further split into 10 folds. Optuna 3 is used to control the tuning of ML methods (see Algorithm 1). X_{train}, y_{train} \\gets next fold in 10-fold cross validation auc \\gets auc.append(est.predict(X_{test}, y_{test})) Both ML methods participating in the duel are optimized 10 times and their performance is averaged on 2 out of 10 folds. This follows the approach presented by pruned-cv 8 . If this difference is positive and significantly better than the previously found ones, all 8 ML methods are evaluated on full 10 folds to determine two optimization factors: difference in AUROC score between 2 competing methods as well as standard deviation between the scores of all 8 ML methods. Otherwise, the potential solution is pruned and invalidated. Having computed optimization factors for all candidates in a given iteration, Pareto-optimal candidates are selected using NSGA-III strategy 6 and corresponding GP-trees are mutated using genetic operators, such as mutations and crossovers.","title":"Stage 1 - pairwise duels"},{"location":"methods.html#algorithm-stage-2-thorough-optimizations","text":"After completing all pairwise duels between ML methods using 15 different random seeds, an extended optimization using 200 combinations of hyper-parameters is performed in order to reduce bias from improperly settings the parameters. Finally, after selecting several hundreds of the most promising solutions, 100 different random seeds are tested, so that the results are independent on the choice of the random seed. The final step includes manual selection of the datasets, which in our opinion represent the most diverse results within the collection.","title":"Algorithm stage 2 - thorough optimizations"},{"location":"methods.html#algorithm-stage-3-filtering-the-best-datasets","text":"In the final steps, a replication of the results is considered. In this stage, for each of the formulas selected, 100 random seeds are used to create different datasets, which are later analyzed using the same model. Each of the datasets is used to run 100 optimizations of each of the 8 ML methods. Based on the results, the collection is narrowed down to 40 datasets.","title":"Algorithm stage 3 - filtering the best datasets"},{"location":"methods.html#operands","text":"In order to determine the dependent variable in each of the datasets in DIGEN40 we used Genetic Programming 7 . Using evolutionary algorithm, in each iteration multiple candidates are tested that feature the following operands: arithmetic: add , subtract , multiply , safe division (safeDiv) ; and relational: greater (or equal) than , lesser (or equal) than , equal , not equal , minimum and maximum . The DIGEN collection is diverse in terms of the choice of the operators included. Only two datasets use exactly the same set of operators (safe division and multiplication), but in different order (either first safeDiv than multiplication, or the opposite).","title":"Operands"},{"location":"methods.html#references","text":"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank Hutter. Efficient and robust automated machine learning. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28 , pages 2962\u20132970. Curran Associates, Inc., 2015. URL: http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf . \u21a9 Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. Auto-sklearn 2.0: the next generation. arXiv preprint arXiv:2007.04074 , 2020. \u21a9 Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: a next-generation hyperparameter optimization framework. In Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 2019. \u21a9 \u21a9 James Bergstra, Brent Komer, Chris Eliasmith, Dan Yamins, and David D Cox. Hyperopt: a python library for model selection and hyperparameter optimization. Computational Science & Discovery , 8 1 :014008, 2015. \u21a9 Jason H Moore, Randal S Olson, Peter Schmitt, Yong Chen, and Elisabetta Manduchi. How computational thought experiments can improve our understanding of the genetic architecture of common human diseases. In Artificial Life Conference Proceedings , 23\u201330. MIT Press, 2018. \u21a9 Kalyanmoy Deb and Himanshu Jain. An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: solving problems with box constraints. IEEE transactions on evolutionary computation , 18 4 :577\u2013601, 2013. \u21a9 John R Koza and John R Koza. Genetic programming: on the programming of computers by means of natural selection . Volume 1. MIT press, 1992. \u21a9 https://github.com/PiotrekGa/pruned-cv \u21a9","title":"References"}]}